{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e0c446-87cc-4e3f-a15e-96efd69bc8ab",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196df3af-4299-40af-845f-f56c9f110b2d",
   "metadata": {},
   "source": [
    "## 1.) Concepts of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6c7831-dd0b-4ba9-bd6a-df4088ec52f8",
   "metadata": {},
   "source": [
    "Let \n",
    "$\\begin{align}\n",
    "    \\mathbf{x} &= \\begin{bmatrix}\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           x_{n}\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}$\n",
    "be the input vector and\n",
    "$\\begin{align}\n",
    "    \\mathbf{y} &= \\begin{bmatrix}\n",
    "           y_{1} \\\\\n",
    "           y_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           y_{n}\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}$\n",
    "be the output (target) vector. <br>\n",
    "Let $L$ be the number of layers excluding input layer (hidden layers + output layer) and $L(\\mathbf{g}(\\mathbf{x}), \\mathbf{y})$ be the loss function. <br>\n",
    "Let $\\textbf{b}^l$ be the bias of $l^{\\text{th}}$ layer. It is indexed by $b^l_j$ which is the bias term for $j^{\\text{th}}$ node of $l^{\\text{th}}$ layer. <br>\n",
    "Let $W^{l}$ be the weight matrix between $l$ and $l-1$ layer. It is indexed by $w_{jk}^l$ where $j$ is a node in the $l^{\\text{th}}$ layer and $k$ is a node in the $(l-1)^{\\text{th}}$ layer. <br>\n",
    "Let $$z_j^{l}=\\sum_k \\left(w_{jk}^l A^{l-1}_k\\right) + b^l_j$$ be the pre-activation of $j^\\text{th}$ node of $l^{\\text{th}}$ layer.\n",
    "\n",
    "Let $\\phi(x)$ be the activation function, then, the activation of $j^\\text{th}$ node of $l^{\\text{th}}$ layer is given by:\n",
    "$$A^l_j = \\phi(z^l_j) = \\phi\\left(\\sum_k \\left(w_{jk}^l A^{l-1}_k\\right) + b^l_j\\right)$$\n",
    "\n",
    "Matrix form of the equations:-\n",
    "$$\\mathbf{Z}^l = W^l\\mathbf{A}^{l-1}+\\mathbf{b}^l$$\n",
    "$$\\mathbf{A}^l = \\phi(\\mathbf{Z}^l) = \\phi(W^l\\mathbf{A}^{l-1}+\\mathbf{b}^l)$$\n",
    "\n",
    "If $\\mathbf{g}(\\mathbf{x}, \\mathbf{W}, \\mathbf{b})$ is the network output, where,\n",
    "$\\begin{align}\n",
    "    \\mathbf{W} &= \\begin{bmatrix}\n",
    "           W^1 \\\\\n",
    "           W^2 \\\\\n",
    "           \\vdots \\\\\n",
    "           W^L\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}$\n",
    "and \n",
    "$\\begin{align}\n",
    "    \\mathbf{b} &= \\begin{bmatrix}\n",
    "           \\mathbf{b}^1 \\\\\n",
    "           \\mathbf{b}^2 \\\\\n",
    "           \\vdots \\\\\n",
    "           \\mathbf{b}^L\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}$\n",
    "then, $\\mathbf{g}(\\mathbf{x}, \\mathbf{W}, \\mathbf{b})=\\mathbf{A}^{L}$, which is the activation of the last layer.\n",
    "Finally, the loss function is given by $L(\\mathbf{g}(\\mathbf{x}, \\mathbf{W}, \\mathbf{b}), \\mathbf{y})$.\n",
    "\n",
    "Note:- If $L$ is the number of hidden layers, then $l \\in \\{1, 2, 3, \\cdots, L\\}$. Furthermore, $l = 0$ can be considered the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c419287-1543-401f-b379-acc65d1a6dfd",
   "metadata": {},
   "source": [
    "## 2.) Backpropagation Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29fe52b-6fbc-4471-8825-d39f63b3fe62",
   "metadata": {},
   "source": [
    "To minimise the loss function, we consider the critical points at which the gradiet of the loss function with respect to the weights and biases vanish. To furthermore prove these points are points of minima (and not maxima) and thus analytically find the optimal values are not possible. Hence we will use the approach of gradient descent. <br>\n",
    "In this method, we will first evaluate the gradient of the loss function with respect to the weights and biases. The negative of this gradient will give the direction of a minimum. We will update the gradients and biases to move in that direction until convergence. From here on, tensorial notation is used.\n",
    "\n",
    "$$\\nabla_{w^l_{jk}, b^l_k} L = \\left(\\frac{\\partial L}{\\partial w^l_{jk}},\\quad \\frac{\\partial L}{\\partial b^l_k}\\right)$$\n",
    "\n",
    "Let us first consider the first term inside the parenthesis. Note that although $L=L(A^L_i)$, $A^L_i$ is itself a function of $A^{L-1}_i$ and so on. So, $A^L_i$ is also a function of $A^l_i$ for some general $l$ and hence we can expand $\\frac{\\partial L}{\\partial w^l_{jk}}$ using chain rule through $A^l_i$. This gives:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w^l_{jk}} = \\sum_p\\frac{\\partial L}{\\partial A^l_p}\\frac{\\partial A^l_p}{\\partial w^l_{jk}}$$\n",
    "$$\\frac{\\partial L}{\\partial w^l_{jk}} = \\sum_p\\frac{\\partial L}{\\partial A^l_p}\\sum_q\\frac{\\partial A^l_p}{\\partial z^l_q}\\frac{\\partial z^l_q}{\\partial w^l_{jk}}$$\n",
    "$$\\frac{\\partial L}{\\partial w^l_{jk}} = \\sum_{p,q}\\frac{\\partial L}{\\partial A^l_p}\\frac{\\partial \\phi(z^l_p)}{\\partial z^l_q}\\frac{\\partial z^l_q}{\\partial w^l_{jk}}$$\n",
    "$$\\frac{\\partial L}{\\partial w^l_{jk}} = \\sum_{p,q}\\frac{\\partial L}{\\partial A^l_p}\\frac{\\partial \\phi(z^l_p)}{\\partial z^l_p}\\frac{\\partial z^l_p}{\\partial z^l_q}\\frac{\\partial z^l_q}{\\partial w^l_{jk}}$$\n",
    "$$\\frac{\\partial L}{\\partial w^l_{jk}} = \\sum_{p,q}\\frac{\\partial L}{\\partial A^l_p}\\phi'(z^l_p)\\delta_{p,q}\\frac{\\partial z^l_q}{\\partial w^l_{jk}}$$\n",
    "$$\\frac{\\partial L}{\\partial w^l_{jk}} = \\sum_{p}\\frac{\\partial L}{\\partial A^l_p}\\phi'(z^l_p)\\frac{\\partial z^l_p}{\\partial w^l_{jk}}$$\n",
    "$$\\frac{\\partial L}{\\partial w^l_{jk}} = \\sum_{p}\\frac{\\partial L}{\\partial A^l_p}\\phi'(z^l_p)\\frac{\\partial}{\\partial w^l_{jk}}\\left(\\sum_s\\left(w^l_{ps}A^{l-1}_s\\right)+b^l_p\\right)$$\n",
    "$$\\frac{\\partial L}{\\partial w^l_{jk}} = \\sum_{p}\\frac{\\partial L}{\\partial A^l_p}\\phi'(z^l_p)\\sum_s\\frac{\\partial}{\\partial w^l_{jk}}(w^l_{ps}A^{l-1}_s)$$\n",
    "$$\\frac{\\partial L}{\\partial w^l_{jk}} = \\sum_{p}\\frac{\\partial L}{\\partial A^l_p}\\phi'(z^l_p)\\sum_s\\left(\\frac{\\partial w^l_{ps}}{\\partial w^l_{jk}}A^{l-1}_s+w^l_{ps}\\frac{\\partial A^{l-1}_s}{\\partial w^l_{jk}}\\right)$$\n",
    "$$\\frac{\\partial L}{\\partial w^l_{jk}} = \\sum_{p}\\frac{\\partial L}{\\partial A^l_p}\\phi'(z^l_p)\\sum_s\\delta_{p,j}\\delta_{s,k} A^{l-1}_s$$\n",
    "$$\\frac{\\partial L}{\\partial w^l_{jk}} = \\sum_{p}\\frac{\\partial L}{\\partial A^l_p}\\phi'(z^l_p)\\delta_{p,j}\\sum_s\\delta_{s,k} A^{l-1}_s$$\n",
    "$$\\frac{\\partial L}{\\partial w^l_{jk}} = \\sum_{p}\\frac{\\partial L}{\\partial A^l_p}\\phi'(z^l_p)\\delta_{p,j}A^{l-1}_k$$\n",
    "$$\\frac{\\partial L}{\\partial w^l_{jk}} = \\frac{\\partial L}{\\partial A^l_j}\\phi'(z^l_j)A^{l-1}_k$$\n",
    "\n",
    "Let $$\\delta^l_j=\\frac{\\partial L}{\\partial A^l_j}\\phi'(z^l_j)$$\n",
    "Then, we have,\n",
    "$$\\frac{\\partial L}{\\partial w^l_{jk}} = \\delta^l_j A^{l-1}_k$$\n",
    "This is the general equation for gradient of loss function with respect to weights. In matrix form, we have,\n",
    "$$\\frac{\\partial L}{\\partial W^l} = \\delta^l \\otimes \\mathbf{A}^{l-1}$$\n",
    "Where\n",
    "$$\\delta^l = \\frac{\\partial L}{\\partial \\mathbf{A}^l}\\odot\\phi'(\\mathbf{Z}^l)$$\n",
    "is the matrix form of definition of $\\delta^l_j$\n",
    "\n",
    "Now, let's look at the value of $\\delta$ for $l=L$.\n",
    "$$\\delta^L =\\frac{\\partial L}{\\partial \\mathbf{A}^L}\\odot\\phi'(\\mathbf{Z}^L)$$\n",
    "But, $\\frac{\\partial L}{\\partial \\mathbf{A}^L} = \\frac{\\partial L}{\\partial \\mathbf{g}}$ where $\\mathbf{g}$ is the network output and thus\n",
    "$$\\delta^L =\\frac{\\partial L}{\\partial \\mathbf{g}}\\odot\\phi'(\\mathbf{Z}^L)$$\n",
    "What about $l<L$? Let's take a look.\n",
    "$$\\delta^l_j= \\frac{\\partial L}{\\partial A^l_j}\\phi'(z^l_j)$$\n",
    "Notice that the only unknown piece is $\\frac{\\partial L}{\\partial A^l_j}$. Let's try to evaluate it. Ideally, since $\\frac{\\partial L}{\\partial A^L_j}$ is known, it would be best to expand this piece as a chain rule of $(l+1)^{\\text{th}}$ layer to iteratively compute it.\n",
    "$$\\frac{\\partial L}{\\partial A^l_j} = \\sum_s\\frac{\\partial L}{\\partial A^{l+1}_s}\\frac{\\partial A^{l+1}_s}{\\partial A^l_j}$$\n",
    "$$\\frac{\\partial L}{\\partial A^l_j} = \\sum_s\\frac{\\partial L}{\\partial A^{l+1}_s}\\sum_r\\frac{\\partial A^{l+1}_s}{\\partial z^{l+1}_r}\\frac{\\partial z^{l+1}_r}{\\partial A^l_j}$$\n",
    "$$\\frac{\\partial L}{\\partial A^l_j} = \\sum_s\\frac{\\partial L}{\\partial A^{l+1}_s}\\sum_r\\phi'(z^{l+1}_s)\\frac{\\partial z^{l+1}_s}{\\partial z^{l+1}_r}\\frac{\\partial z^{l+1}_r}{\\partial A^l_j}$$\n",
    "$$\\frac{\\partial L}{\\partial A^l_j} = \\sum_s\\frac{\\partial L}{\\partial A^{l+1}_s}\\sum_r\\phi'(z^{l+1}_s)\\delta_{r,s}\\frac{\\partial z^{l+1}_r}{\\partial A^l_j}$$\n",
    "$$\\frac{\\partial L}{\\partial A^l_j} = \\sum_s\\frac{\\partial L}{\\partial A^{l+1}_s}\\phi'(z^{l+1}_s)\\frac{\\partial z^{l+1}_s}{\\partial A^l_j}$$\n",
    "$$\\frac{\\partial L}{\\partial A^l_j} = \\sum_s\\frac{\\partial L}{\\partial A^{l+1}_s}\\phi'(z^{l+1}_s)\\frac{\\partial}{\\partial A^l_j}\\left(\\sum_p\\left(w^{l+1}_{sp}A^l_p\\right)+b^{l+1}_s\\right)$$\n",
    "$$\\frac{\\partial L}{\\partial A^l_j} = \\sum_s\\frac{\\partial L}{\\partial A^{l+1}_s}\\phi'(z^{l+1}_s)\\sum_p\\frac{\\partial}{\\partial A^l_j}(w^{l+1}_{sp}A^l_p)$$\n",
    "$$\\frac{\\partial L}{\\partial A^l_j} = \\sum_s\\frac{\\partial L}{\\partial A^{l+1}_s}\\phi'(z^{l+1}_s)\\sum_p w^{l+1}_{sp}\\frac{\\partial A^l_p}{\\partial A^l_j}$$\n",
    "$$\\frac{\\partial L}{\\partial A^l_j} = \\sum_s\\frac{\\partial L}{\\partial A^{l+1}_s}\\phi'(z^{l+1}_s)\\sum_p w^{l+1}_{sp}\\delta_{p,j}$$\n",
    "$$\\frac{\\partial L}{\\partial A^l_j} = \\sum_s\\frac{\\partial L}{\\partial A^{l+1}_s}\\phi'(z^{l+1}_s)w^{l+1}_{sj}$$\n",
    "Notice that the first two factors inside the summation are precisely $\\delta^{l+1}_s$!\n",
    "$$\\frac{\\partial L}{\\partial A^l_j} = \\sum_s\\delta^{l+1}_sw^{l+1}_{sj}$$\n",
    "Thus,\n",
    "$$\\delta^l_j= \\sum_s\\left(\\delta^{l+1}_sw^{l+1}_{sj}\\right)\\phi'(z^l_j)$$\n",
    "In matrix form,\n",
    "$$\\delta^l= \\left(W^{l+1}_T\\delta^{l+1}\\right)\\odot\\phi'(\\mathbf{Z}^l)$$\n",
    "Finally, we have,\n",
    "$$\\frac{\\partial L}{\\partial W^l} = \\delta^l \\otimes \\mathbf{A}^{l-1}, \\quad \\delta^l =\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      \\frac{\\partial L}{\\partial \\mathbf{g}}\\odot\\phi'(\\mathbf{Z}^L) & l=L \\\\\n",
    "      (W^{l+1}_T\\delta^{l+1})\\odot\\phi'(\\mathbf{Z}^l) & l<L\n",
    "\\end{array}\n",
    "\\right.$$\n",
    "In tensor form,\n",
    "$$\\frac{\\partial L}{\\partial w^l_{jk}} = \\delta^l_j\\mathbf{A}^{l-1}_k, \\quad \\delta^l_j =\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      \\frac{\\partial L}{\\partial g_j}\\phi'(z^L_j) & l=L \\\\\n",
    "      \\sum_s\\left(w^{l+1}_{sj}\\delta^{l+1}_s\\right)\\phi'(z^l_j) & l<L\n",
    "\\end{array}\n",
    "\\right.$$\n",
    "\n",
    "Well, what about the bias term? A simple inspection at the derivation for gradient with respect to the weights reveals the following result for the bias term:\n",
    "\n",
    "In matrix form,\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{b}^l} = \\delta^l, \\quad \\delta^l =\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      \\frac{\\partial L}{\\partial \\mathbf{g}}\\odot\\phi'(\\mathbf{Z}^L) & l=L \\\\\n",
    "      (W^{l+1}_T\\delta^{l+1})\\odot\\phi'(\\mathbf{Z}^l) & l<L\n",
    "\\end{array}\n",
    "\\right.$$\n",
    "In tensor form,\n",
    "$$\\frac{\\partial L}{\\partial b^l_j} = \\delta^l_j, \\quad \\delta^l_j =\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      \\frac{\\partial L}{\\partial g_j}\\phi'(z^L_j) & l=L \\\\\n",
    "      \\sum_s\\left(w^{l+1}_{sj}\\delta^{l+1}_s\\right)\\phi'(z^l_j) & l<L\n",
    "\\end{array}\n",
    "\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8336350-1a85-425b-88ad-8c40f78c3076",
   "metadata": {},
   "source": [
    "## 3.) Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b19213b-038d-48d2-b78c-ae22b7c280fe",
   "metadata": {},
   "source": [
    "Let us consider the case of a convolution layer now. Now, instead of an input vector, we have an input tensor of size $H\\times W\\times C$.\n",
    "Where $H$ is the number of rows, $W$ is the number of columns and $C$ is the number of channels. Typically, $C$ = 3 for red, green and blue, otherwise $C$ = 1 for grayscale images. We have the input tensor\n",
    "$$X = X_{ijk}$$\n",
    "Here $i\\in\\{1,2,\\cdots,H\\}$, <br>$j\\in\\{1,2,\\cdots,W\\}$, <br>$k\\in\\{1,2,\\cdots,C\\}$ are the tensor indexes for the height, width and channels respectively.\n",
    "\n",
    "In the convolution layer, the input tensor is convolved with kernels (called filters) which are used to pick up spatial features. Typically, a kernel is a square matrix of weights of size $V\\times V$. First, I define the kernel tensor,\n",
    "$$K=K_{ijkf}$$\n",
    "Where, $i\\in\\{1,2,\\cdots,V\\}$, <br>$j\\in\\{1,2,\\cdots,V\\}$, <br> $k\\in\\{1,2,\\cdots,C\\}$, <br>$f\\in\\{1,2,\\cdots,F\\}$. <br><br>\n",
    "\n",
    "The first two indexes specify the weight in the $i^{\\text{th}}$ and $j^{\\text{th}}$ position for $k^\\text{th}$ color of the $f^\\text{th}$ kernel. <br>\n",
    "We define the convolution operator which gives the features due to convolution with the $f^{\\text{th}}$ filter. The color channels are summed and combined. It may be noted that the step size of $m$ and $n$ may not be, in general, $1$. Their step size is in fact called the stride $S$ of the kernels.\n",
    "$$X^\\sim_{ijf} = X*K = \\sum_{k=1}^C\\sum_n^V\\sum_m^V X_{i+m,j+n,k}K_{mnkf}$$\n",
    "\n",
    "The size of $X^\\sim_{ijf}$ is given by\n",
    "$$H_\\text{conv} = \\frac{H-V+2P}{S} + 1$$\n",
    "$$W_\\text{conv} = \\frac{W-V+2P}{S} + 1$$\n",
    "\n",
    "Thus the activation becomes:\n",
    "$$A_{ijf} = \\phi(X^\\sim_{ijf}+b_f)$$\n",
    "\n",
    "Note that instead of a bias tensor $B_{ijf}$, we have a bias vector $b_f$, because we consider $B_{ijf} = b_f$ for all pairs of $i$ and $j$.\n",
    "\n",
    "After the convolution layer, the signal is pooled to reduce dimensionality.\n",
    "$$A^\\sim_{xyz}=\\text{pool}(A_{ijf})$$\n",
    "Mathematically, the pooling operation would be realized as a $6^\\text{th}$ rank tensor.\n",
    "$$A^\\sim_{xyz}=\\sum_{ijf}Q_{xyzijf}A_{ijf}$$\n",
    "After that, the output of the pooled layer is flattened and fed as input to a dense network.\n",
    "$$A'_p = \\text{flatten}(A^\\sim_{xyz})$$\n",
    "By similar logic, the flattening tensor consists of a $4^\\text{th}$ rank tensor.\n",
    "$$A'_p=\\sum_{xyz}R_{pxyz}A^\\sim_{xyz}$$\n",
    "\n",
    "Now,\n",
    "$$\\frac{\\partial L}{\\partial K_{bgtv}} = \\sum_p\\frac{\\partial L}{\\partial A'_p}\\frac{\\partial A'_p}{\\partial K_{bgtv}}$$\n",
    "\n",
    "But, $A'p$ is now basically the input of a dense network, so we actually know the first factor. It is actually $\\delta^0_p=\\sum_s\\left(w^1_{sp}\\delta^1_s\\right)\\phi'(z^1_p)$ \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial K_{bgtv}} = \\sum_p \\delta^0_p\\frac{\\partial A'_p}{\\partial K_{bgtv}}$$\n",
    "$$\\frac{\\partial L}{\\partial K_{bgtv}} = \\sum_p \\delta^0_p\\frac{\\partial }{\\partial K_{bgtv}}\\left(\\sum_{xyz}R_{pxyz}A^\\sim_{xyz}\\right)$$\n",
    "$$\\frac{\\partial L}{\\partial K_{bgtv}} = \\sum_{pxyz} \\delta^0_p R_{pxyz}\\frac{\\partial A^\\sim_{xyz}}{\\partial K_{bgtv}}$$\n",
    "$$\\frac{\\partial L}{\\partial K_{bgtv}} = \\sum_{pxyz} \\delta^0_p R_{pxyz}\\frac{\\partial }{\\partial K_{bgtv}}\\left(\\sum_{ijf}Q_{xyzijf}A_{ijf}\\right)$$\n",
    "$$\\frac{\\partial L}{\\partial K_{bgtv}} = \\sum_{pxyzijf} \\delta^0_p R_{pxyz}Q_{xyzijf}\\frac{\\partial A_{ijf}}{\\partial K_{bgtv}}$$\n",
    "$$\\frac{\\partial L}{\\partial K_{bgtv}} = \\sum_{pxyzijf} \\delta^0_p R_{pxyz}Q_{xyzijf}\\phi'(X^\\sim_{ijf}+b_f)\\frac{\\partial X^\\sim_{ijf}}{\\partial K_{bgtv}}$$\n",
    "$$\\frac{\\partial L}{\\partial K_{bgtv}} = \\sum_{pxyzijf} \\delta^0_p R_{pxyz}Q_{xyzijf}\\phi'(X^\\sim_{ijf}+b_f)\\sum_{knm}X_{i+m, j+n,k}\\frac{\\partial K_{ijkf}}{\\partial K_{bgtv}}$$\n",
    "$$\\frac{\\partial L}{\\partial K_{bgtv}} = \\sum_{pxyzijf} \\delta^0_p R_{pxyz}Q_{xyzijf}\\phi'(X^\\sim_{ijf}+b_f)\\sum_{knm}X_{i+m, j+n,k}\\delta_{i,b}\\delta_{j,g}\\delta_{k,t}\\delta_{f,v}$$\n",
    "$$\\frac{\\partial L}{\\partial K_{bgtv}} = \\sum_{pxyz} \\delta^0_p R_{pxyz}Q_{xyzbgv}\\phi'(X^\\sim_{bgv}+b_v)\\sum_{nm}X_{b+m, g+n,t}$$\n",
    "$$\\frac{\\partial L}{\\partial K_{bgtv}} = \\left(\\sum_p\\delta^0_pR_{pxyz}\\right)\\left(\\sum_{xyz}Q_{xyzbgv}\\right)\\phi'(X^\\sim_{bgv}+b_v)\\sum_{nm}X_{b+m,g+n,t}$$\n",
    "The first factor represents de-flattening, the second factor represents de-pooling. These can be pulled back from the flattening and pooling layers respectively. The last factor is an elementwise product returned by the convolution layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56997630-85ff-4fc1-8080-f77a12bdd3b4",
   "metadata": {},
   "source": [
    "## 4.) The Computational Graph Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a75a1c-2dd6-4109-8659-a5148b0069a6",
   "metadata": {},
   "source": [
    "As one can see, the computation of gradients becomes really non-trivial for more complex models. In these cases, it is better to follow a computational graph approach. In this approach, each layer can be thought of as a node in a graph. Each layer has some edges flowing in (such as input, weights) which determine the output edge. Each layer then receives the gradient from the next layer through the output edge and flows back its own local gradients with respect to output backwards along the input edges.\n",
    "\n",
    "Let us consider a dense layer in this context. A dense layer has three input edges, $X$ which is the output from the previous layer and $W$ which is the weight matrix and $B$ which are the biases. These three inputs are enough to determine the output $O$ of the layer, which is sent via the output edge. Furthermore, the layer receives the gradient backflow $\\frac{\\partial L}{\\partial O}$ from the output edge. The layer's job is then, to send its version of gradients back through the input edge. These can be easily evaluated using the backflow the layer itself has received:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial O}\\frac{\\partial O}{\\partial X}$$\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial O}\\frac{\\partial O}{\\partial W}$$\n",
    "$$\\frac{\\partial L}{\\partial B} = \\frac{\\partial L}{\\partial O}\\frac{\\partial O}{\\partial B}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ab469f-c33d-4973-b648-2dd80ff7d87e",
   "metadata": {},
   "source": [
    "### 4a.) Dense Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7efe9c3-a782-41c9-b179-69c64a21dbae",
   "metadata": {},
   "source": [
    "For a dense network, we have:\n",
    "Gradient with respect to input:\n",
    "$$\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial O}\\frac{\\partial O}{\\partial X}$$\n",
    "In tensor form, we have,\n",
    "$$\\frac{\\partial L}{\\partial X_i} = \\sum_j\\frac{\\partial L}{\\partial O_j}\\frac{\\partial O_j}{\\partial X_i}$$\n",
    "$$\\frac{\\partial L}{\\partial X_i} = \\sum_j\\frac{\\partial L}{\\partial O_j}\\phi'(Z_j)\\frac{\\partial}{\\partial X_i}\\left(\\sum_k W_{jk}X_k+B_j\\right)$$\n",
    "Where, $Z_j = \\sum_k W_{jk}X_k+B_j$\n",
    "$$\\frac{\\partial L}{\\partial X_i} = \\sum_j\\frac{\\partial L}{\\partial O_j}\\phi'(Z_j)\\sum_k W_{jk}\\frac{\\partial X_k}{\\partial X_i}$$\n",
    "$$\\frac{\\partial L}{\\partial X_i} = \\sum_j\\frac{\\partial L}{\\partial O_j}\\phi'(Z_j)\\sum_k W_{jk}\\delta_{ik}$$\n",
    "$$\\frac{\\partial L}{\\partial X_i} = \\sum_j\\frac{\\partial L}{\\partial O_j}\\phi'(z_j)W_{ji}$$\n",
    "$$\\frac{\\partial L}{\\partial X_i} = \\sum_jW_{ij}^T\\frac{\\partial L}{\\partial O_j}\\phi'(Z_j)$$\n",
    "Or, in matrix form,\n",
    "$$\\frac{\\partial L}{\\partial X} = W^T\\left(\\frac{\\partial L}{\\partial O}\\odot\\phi'(Z)\\right)$$\n",
    "Gradient with respect to weights:\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial O}\\frac{\\partial O}{\\partial W}$$\n",
    "In tensor form, we have,\n",
    "$$\\frac{\\partial L}{\\partial W_{ij}} = \\sum_k\\frac{\\partial L}{\\partial O_k}\\frac{\\partial O_k}{\\partial W_{ij}}$$\n",
    "$$\\frac{\\partial L}{\\partial W_{ij}} = \\sum_k\\frac{\\partial L}{\\partial O_k}\\phi'(Z_k)\\frac{\\partial}{\\partial W_{ij}}\\left(\\sum_p W_{kp} X_p + B_k\\right)$$\n",
    "$$\\frac{\\partial L}{\\partial W_{ij}} = \\sum_k\\frac{\\partial L}{\\partial O_k}\\phi'(Z_k)\\sum_p \\frac{\\partial W_{kp}}{\\partial W_{ij}} X_p$$\n",
    "$$\\frac{\\partial L}{\\partial W_{ij}} = \\sum_k\\frac{\\partial L}{\\partial O_k}\\phi'(Z_k)\\sum_p \\delta_{ki}\\delta_{pj} X_p$$\n",
    "$$\\frac{\\partial L}{\\partial W_{ij}} = \\sum_k\\frac{\\partial L}{\\partial O_k}\\phi'(Z_k) \\delta_{ki} X_j$$\n",
    "$$\\frac{\\partial L}{\\partial W_{ij}} = \\frac{\\partial L}{\\partial O_i}\\phi'(Z_i) X_j$$\n",
    "Or, in matrix form,\n",
    "$$\\frac{\\partial L}{\\partial W} = \\left(\\frac{\\partial L}{\\partial O}\\odot\\phi'(Z)\\right) \\otimes X = \\left(\\frac{\\partial L}{\\partial O}\\odot\\phi'(Z)\\right)X^T$$\n",
    "Gradient with respect to biases:\n",
    "$$\\frac{\\partial L}{\\partial B} = \\frac{\\partial L}{\\partial O}\\frac{\\partial O}{\\partial B}$$\n",
    "In tensor form, we have,\n",
    "$$\\frac{\\partial L}{\\partial B_i} = \\sum_j\\frac{\\partial L}{\\partial O_j}\\frac{\\partial O_j}{\\partial B_i}$$\n",
    "$$\\frac{\\partial L}{\\partial B_i} = \\sum_j\\frac{\\partial L}{\\partial O_j}\\phi'(Z_j)\\frac{\\partial}{\\partial B_i}\\left(\\sum_k W_{jk}X_k +B_j\\right)$$\n",
    "$$\\frac{\\partial L}{\\partial B_i} = \\sum_j\\frac{\\partial L}{\\partial O_j}\\phi'(Z_j)\\frac{\\partial B_j}{\\partial B_i}$$\n",
    "$$\\frac{\\partial L}{\\partial B_i} = \\sum_j\\frac{\\partial L}{\\partial O_j}\\phi'(Z_j)\\delta_{ij}$$\n",
    "$$\\frac{\\partial L}{\\partial B_i} = \\frac{\\partial L}{\\partial O_i}\\phi'(Z_i)$$\n",
    "Or, in matrix form,\n",
    "$$\\frac{\\partial L}{\\partial B} = \\frac{\\partial L}{\\partial O}\\odot\\phi'(Z)$$\n",
    "Summarizing,\n",
    "$$\\frac{\\partial L}{\\partial X} = W^T\\left(\\frac{\\partial L}{\\partial O}\\odot\\phi'(Z)\\right)$$\n",
    "$$\\frac{\\partial L}{\\partial W} = \\left(\\frac{\\partial L}{\\partial O}\\odot\\phi'(Z)\\right) \\otimes X = \\left(\\frac{\\partial L}{\\partial O}\\odot\\phi'(Z)\\right)X^T$$\n",
    "$$\\frac{\\partial L}{\\partial B} = \\frac{\\partial L}{\\partial O}\\odot\\phi'(Z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f862173-81a4-4d63-a23a-20ef8e7cbe4e",
   "metadata": {},
   "source": [
    "### 4b.) Convolution Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc4a262-a441-417a-a83b-0af23e650ebe",
   "metadata": {},
   "source": [
    "The flattening layer has a single input X and the output O is just reshaped X.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial O}\\frac{\\partial O}{\\partial X}$$\n",
    "Again, we briefly switch to tensor notation to derive a formula\n",
    "$$\\frac{\\partial L}{\\partial X_{ijk}} = \\sum_h \\frac{\\partial L}{\\partial O_h}\\frac{\\partial O_h}{\\partial X_{ijk}}$$\n",
    "$$\\frac{\\partial L}{\\partial X_{ijk}} = \\sum_h \\frac{\\partial L}{\\partial O_h}R_{hijk}$$\n",
    "Where $R_{hijk}$ is the flattening tensor.\n",
    "$$R_{hijk}=\\delta_{h, iJK+jK+k}$$\n",
    "$$\\frac{\\partial L}{\\partial X_{ijk}} = \\sum_h \\frac{\\partial L}{\\partial O_h}\\delta_{h, iJK+jK+k}$$\n",
    "$$\\frac{\\partial L}{\\partial X_{ijk}} = \\frac{\\partial L}{\\partial O_{iJK+jK+k}}$$\n",
    "We note that components of $\\frac{\\partial L}{\\partial X}$ are made by selecting specific components of $\\frac{\\partial L}{\\partial O}$, so $\\frac{\\partial L}{\\partial X}$ is just $\\frac{\\partial L}{\\partial O}$ reshaped.\n",
    "In matrix form, then,\n",
    "$$\\frac{\\partial L}{\\partial X} = \\text{reshape}\\left(\\frac{\\partial L}{\\partial O}\\right)$$<br>\n",
    "The pooling layer also has a single input X. Thus, we have,\n",
    "$$\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial O}\\frac{\\partial O}{\\partial X}$$\n",
    "In tensor form,\n",
    "$$\\frac{\\partial L}{\\partial X_{abc}} = \\sum_{ijk}\\frac{\\partial L}{\\partial O_{ijk}}\\frac{\\partial O_{ijk}}{\\partial X_{abc}}$$\n",
    "$$\\frac{\\partial L}{\\partial X_{abc}} = \\sum_{ijk}\\frac{\\partial L}{\\partial O_{ijk}}Q_{ijkabc}$$\n",
    "Typically, pooling is done channel-wise. The pooling on one channel is independent of another. So we can write\n",
    "$$Q_{ijkabc}=Q_{ijab}\\delta_{kc}$$\n",
    "$$\\frac{\\partial L}{\\partial X_{abc}} = \\sum_{ijk}\\frac{\\partial L}{\\partial O_{ijk}}Q_{ijab}\\delta_{kc}$$\n",
    "$$\\frac{\\partial L}{\\partial X_{abc}} = \\sum_{ij}\\frac{\\partial L}{\\partial O_{ijc}}Q_{ijab}$$\n",
    "Now, $Q_{ijab}$ is the upsampling tensor. It unpools the output to reconstruct the tensor before pooling.\n",
    "In matrix form, then,\n",
    "$$\\frac{\\partial L}{\\partial X}=\\text{unpool}\\left(\\frac{\\partial L}{\\partial O}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14177a7e-5476-4c93-8977-2c5ce56c3827",
   "metadata": {},
   "source": [
    "## 6.) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7aa48db9-322e-4053-9eec-893ddbc127ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import random\n",
    "from IPython.display import display\n",
    "from numpy.lib.stride_tricks import sliding_window_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c92137cb-29e2-413f-9152-57bd86044ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(x):\n",
    "    return sigmoid(x)*(1.0 - sigmoid(x))\n",
    "\n",
    "def min_sq(g, y):\n",
    "    return 0.5*np.sum((g - y)**2)\n",
    "\n",
    "def dmin_sq(g, y):\n",
    "    return g - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9dc6f40b-a7e2-4c25-ab25-f419eddccca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, shape, af, daf):\n",
    "        self.W = random.uniform(0.0, 1.0, shape[::-1])\n",
    "        self.B = random.uniform(0.0, 1.0, (shape[1], 1))\n",
    "        self.af = af\n",
    "        self.daf = daf\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.X = x\n",
    "        self.Z = self.W @ self.X + self.B\n",
    "        self.O = self.af(self.Z)\n",
    "        return self.O\n",
    "\n",
    "    def backward(self, delta, lr):\n",
    "        dB = delta * self.daf(self.Z)\n",
    "        dW = dB @ self.X.T\n",
    "        dX = self.W.T @ dB\n",
    "        self.W -= lr*dW\n",
    "        self.B -= lr*dB\n",
    "        return dX\n",
    "\n",
    "class FlatLayer:\n",
    "    def forward(self, x):\n",
    "        self.X = x\n",
    "        self.O = self.X.reshape(-1, 1)\n",
    "        return self.O\n",
    "\n",
    "    def backward(self, delta, lr=None):\n",
    "        dX = delta.reshape(self.X.shape)\n",
    "        return dX\n",
    "\n",
    "class PoolingLayer:\n",
    "    def __init__(self, stride, pool_shape, pool_method, unpool_method):\n",
    "        if isinstance(stride, int):\n",
    "            self.stride = (stride, stride)\n",
    "        else:\n",
    "            self.stride = stride\n",
    "        if isinstance(pool_shape, int):\n",
    "            self.pool_shape = (pool_shape, pool_shape)\n",
    "        else:\n",
    "            self.pool_shape = pool_shape\n",
    "        self.pool_method = pool_method\n",
    "        self.unpool_method = unpool_method\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.X = x\n",
    "        windows = sliding_window_view(x, window_shape=self.pool_shape, axis=(0,1))\n",
    "        windows = windows[::self.stride[0], ::self.stride[1], :, :, :]\n",
    "        if self.pool_method == \"max\":\n",
    "            self.O = windows.max(axis=(3,4))\n",
    "        elif self.pool_method in [\"mean\", \"avg\"]:\n",
    "            self.O = windows.mean(axis=(3,4))\n",
    "\n",
    "        #argmax caching\n",
    "        self.argmax = windows.reshape(*windows[:3], -1).argmax(-1)\n",
    "        \n",
    "        return self.O\n",
    "\n",
    "    def backward(self, delta, lr=None):\n",
    "        dX = np.zeros_like(self.X)\n",
    "        a = np.arange(delta.shape[0])[:, None, None] * self.stride[0] + self.argmax // self.pool_shape[1]\n",
    "        b = np.arange(delta.shape[1])[None, :, None] * self.stride[1] + self.argmax %  self.pool_shape[1]\n",
    "        c = np.arange(delta.shape[2])[None, None, :]\n",
    "        np.add.at(dX, (a, b, c), delta)\n",
    "        return dX\n",
    "\n",
    "class ConvLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, layers, L, dL):\n",
    "        self.layers = layers\n",
    "        self.L = L\n",
    "        self.dL = dL\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        self.output = x\n",
    "\n",
    "    def backward(self, target, lr):\n",
    "        delta = self.dL(self.output, target)\n",
    "        for layer in self.layers[::-1]:\n",
    "            delta = layer.backward(delta, lr)\n",
    "\n",
    "    def train(self, x, target, epochs, lr):\n",
    "        for t in range(epochs):\n",
    "            self.forward(x)\n",
    "            self.backward(target, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea508f68-8237-47f9-aa23-66c6f8430851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_layer</th>\n",
       "      <th>layer1</th>\n",
       "      <th>layer2</th>\n",
       "      <th>output_layer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.995462</td>\n",
       "      <td>0.734062</td>\n",
       "      <td>0.801021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.981624</td>\n",
       "      <td>0.842078</td>\n",
       "      <td>0.901725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.835657</td>\n",
       "      <td>0.899424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.795055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.774896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   input_layer    layer1    layer2  output_layer\n",
       "0          5.0  0.995462  0.734062      0.801021\n",
       "1          3.0  0.981624  0.842078      0.901725\n",
       "2          1.0       NaN  0.835657      0.899424\n",
       "3          7.0       NaN       NaN      0.795055\n",
       "4          NaN       NaN       NaN      0.774896"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_layer</th>\n",
       "      <th>layer1</th>\n",
       "      <th>layer2</th>\n",
       "      <th>output_layer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.995708</td>\n",
       "      <td>0.794701</td>\n",
       "      <td>0.063555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.985876</td>\n",
       "      <td>0.897649</td>\n",
       "      <td>0.951607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.858790</td>\n",
       "      <td>0.066587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.944312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.943120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   input_layer    layer1    layer2  output_layer\n",
       "0          5.0  0.995708  0.794701      0.063555\n",
       "1          3.0  0.985876  0.897649      0.951607\n",
       "2          1.0       NaN  0.858790      0.066587\n",
       "3          7.0       NaN       NaN      0.944312\n",
       "4          NaN       NaN       NaN      0.943120"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_layer</th>\n",
       "      <th>layer1</th>\n",
       "      <th>layer2</th>\n",
       "      <th>output_layer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.995832</td>\n",
       "      <td>0.823952</td>\n",
       "      <td>0.041799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.987533</td>\n",
       "      <td>0.909252</td>\n",
       "      <td>0.964214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.875201</td>\n",
       "      <td>0.042998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.961442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.960663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   input_layer    layer1    layer2  output_layer\n",
       "0          5.0  0.995832  0.823952      0.041799\n",
       "1          3.0  0.987533  0.909252      0.964214\n",
       "2          1.0       NaN  0.875201      0.042998\n",
       "3          7.0       NaN       NaN      0.961442\n",
       "4          NaN       NaN       NaN      0.960663"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.array([5, 3, 1, 7]).reshape(-1, 1)\n",
    "y = np.array([0, 1, 0, 1, 1]).reshape(-1, 1)\n",
    "my_nn = Network([DenseLayer((4,2), sigmoid, dsigmoid),\n",
    "                 DenseLayer((2,3), sigmoid, dsigmoid),\n",
    "                 DenseLayer((3,5), sigmoid, dsigmoid)], min_sq, dmin_sq)\n",
    "my_nn.train(x, y, 101, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a6658150-1268-479c-a3db-0733e67cba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = random.randint(low=0, high=255, size=(5,5,3))\n",
    "windows = sliding_window_view(test, (2, 2), axis=(0, 1))\n",
    "windows = windows[::1,::2,:,:,:]\n",
    "pooled = windows.max(axis=(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "03170e23-d1db-4f58-965e-48211892e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax = windows.reshape(\n",
    "            *windows.shape[:3], -1\n",
    "        ).argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1ad93cf8-16c1-4da9-9295-8ed255080721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[248,  69, 201,  64,  80],\n",
       "       [ 43,  50, 176, 201,  98],\n",
       "       [156, 220, 215,  41, 148],\n",
       "       [ 20,  57, 188, 114, 222],\n",
       "       [180,  69,   8, 189,  14]], dtype=int32)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d73e8023-c365-45b4-88ed-151fa8fd5024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[248, 201],\n",
       "       [220, 215],\n",
       "       [220, 215],\n",
       "       [180, 189]], dtype=int32)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "bbd97393-61b9-458e-94b8-6ecdaa5014c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [3, 2],\n",
       "       [1, 0],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argmax[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "c4705096-93f3-4e25-bae7-c36a3ab4f4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[248.,   0., 201.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.],\n",
       "       [  0., 440., 430.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.],\n",
       "       [180.,   0.,   0., 189.,   0.]])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_construction = np.zeros(shape=(5,5,3))\n",
    "a = argmax // 2\n",
    "b = argmax %  2\n",
    "\n",
    "i = np.arange(pooled.shape[0])[:, None, None]\n",
    "j = np.arange(pooled.shape[1])[None, :, None]\n",
    "c = np.arange(pooled.shape[2])[None, None, :]\n",
    "\n",
    "X_i = i * 1 + a\n",
    "X_j = j * 2 + b\n",
    "\n",
    "np.add.at(re_construction, (X_i, X_j, c), pooled)\n",
    "re_construction[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "8b2e75b0-b351-4f69-ae4d-75658fb700f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [3, 2],\n",
       "       [1, 0],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argmax[:,:,0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
